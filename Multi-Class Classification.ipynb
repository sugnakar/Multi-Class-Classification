{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Automatic Product Title Tagging Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the packages and classes required for pre-processing, modelling etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time\n",
    "import numpy,os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the train, test and evaluation data from the files into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sugnakar/Rocky/BigData/Downloads/m2 zip/CPEE_Batch26_Scholarship_Exam/data'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(\"Train.csv\")\n",
    "test_raw = pd.read_csv(\"Test.csv\")\n",
    "eval_raw = pd.read_csv(\"EvalData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Copying the test and train data to other variables so that original data is not disturbed\n",
    "train_data = train_raw.copy()\n",
    "test_data = test_raw.copy()\n",
    "eval_data = eval_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do anything lets take a look at training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptitle</th>\n",
       "      <th>brand_title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56493</td>\n",
       "      <td>56493</td>\n",
       "      <td>56493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>54799</td>\n",
       "      <td>475</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>?????????</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Mobile &amp; Tablets_Tablet and Smartphone Accesso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>802</td>\n",
       "      <td>13778</td>\n",
       "      <td>14233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ptitle brand_title  \\\n",
       "count       56493       56493   \n",
       "unique      54799         475   \n",
       "top     ?????????       OTHER   \n",
       "freq          802       13778   \n",
       "\n",
       "                                                   target  \n",
       "count                                               56493  \n",
       "unique                                                102  \n",
       "top     Mobile & Tablets_Tablet and Smartphone Accesso...  \n",
       "freq                                                14233  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Based on the above output we can understand that there are 802 records that have the title \"?????????\". Now we have two choices. \n",
    "1.Either we ignore the data by deleting these rows\n",
    "2.Replace the ptitle for all these rows as \"dummytitle\".\n",
    "\n",
    "As we do not have the customer to ask, because this is an exam :-), lets just replace the title values with \"DummyTitle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all the rows that have just \"??????????\" as they show as the top in the data description\n",
    "train_data = train_raw.copy()\n",
    "train_data['ptitle'] = train_data['ptitle'].replace(\"?????????\",\" DummyTitle \")\n",
    "\n",
    "train_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During classification numbers in the data are not of much help. We can do two things - either replace all numbers with a dummy string or replace it with empty.\n",
    "\n",
    "Note: This would be a question to the customer if the number are of any relevance during the classification\n",
    "\n",
    "Lets replace all the numbers of digits >2 with the string \" suspectnumber \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_replacer = \" suspectnumber \"\n",
    "\n",
    "train_data[\"ptitle\"] = train_data[\"ptitle\"].str.replace(\"\\d{2,}\", number_replacer)\n",
    "test_data[\"ptitle\"] = test_data[\"ptitle\"].str.replace(\"\\d{2,}\", number_replacer)\n",
    "eval_data[\"ptitle\"] = eval_data[\"ptitle\"].str.replace(\"\\d{2,}\", number_replacer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in train, test and evaluate have lots of special characters which does not do any help during the classification. Lets replace all the values other than numbers, strings, . with empty character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets replace all the special characters and only consider numbers and english characters\n",
    "train_data[\"ptitle\"] = train_data[\"ptitle\"].str.replace('[^a-zA-Z0-9 \\n\\.]',\"\")\n",
    "test_data[\"ptitle\"] = test_data[\"ptitle\"].str.replace('[^a-zA-Z0-9 \\n\\.]',\"\")\n",
    "eval_data[\"ptitle\"] = eval_data[\"ptitle\"].str.replace('[^a-zA-Z0-9 \\n\\.]',\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing the data clean up lets check the description of the data. Also (optionally) dump the modified data into a file and observe if the data clean up is done properly or removed any key information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ptitle</th>\n",
       "      <th>brand_title</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56493</td>\n",
       "      <td>56493</td>\n",
       "      <td>56493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>52348</td>\n",
       "      <td>475</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>DummyTitle</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>Mobile &amp; Tablets_Tablet and Smartphone Accesso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>802</td>\n",
       "      <td>13778</td>\n",
       "      <td>14233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ptitle brand_title  \\\n",
       "count          56493       56493   \n",
       "unique         52348         475   \n",
       "top      DummyTitle        OTHER   \n",
       "freq             802       13778   \n",
       "\n",
       "                                                   target  \n",
       "count                                               56493  \n",
       "unique                                                102  \n",
       "top     Mobile & Tablets_Tablet and Smartphone Accesso...  \n",
       "freq                                                14233  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data.to_csv(\"modified.csv\")\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the problem statement only title and target columns are relevant. Lets copy them to seperate varaibles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_title_data = train_data[\"ptitle\"]\n",
    "train_target_data = train_data[\"target\"]\n",
    "test_title_data = test_data[\"ptitle\"]\n",
    "test_target_data = test_data[\"target\"]\n",
    "eval_title_data = eval_data[\"ptitle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the transformation of the original title data. This can be done using CountVectorizer or TfidfVectorizer. Tried with both of them but CountVectorizer helped in getting better accuracy and recall values compared to TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#vector = CountVectorizer(ngram_range=(1,2))\n",
    "vector = CountVectorizer()\n",
    "\n",
    "#vector = TfidfVectorizer(ngram_range=(1,2), max_df=0.95, min_df=2, stop_words=['english','german']) \n",
    "#vector = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to tranform the training title data using fit_transform method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_trans=vector.fit_transform(train_title_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many features are derived into the vector based on the training titles . Also to understand what are the various feature names just print them and see. This would also help in further pre-processing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35276\n"
     ]
    }
   ],
   "source": [
    "print len(vector.get_feature_names())\n",
    "#print(vector.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the test data using the same CountVectorizer object. Remember you should use \"tranform\" method and not \"fit_transform\" as want to use the same features identified by the training data to be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<56493x35276 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 563817 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tran = vector.transform(test_title_data)\n",
    "test_data_tran\n",
    "#test_title_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranform the evaluation data using the same CountVectorizer object. Remember you should use \"tranform\" method and not \"fit_transform\" as want to use the same features identified by the training data to be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x35276 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 101709 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data_tran = vector.transform(eval_title_data)\n",
    "#vector.get_feature_names()\n",
    "eval_data_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its time to build a model and test how the model predicts the data on the test and also on the evaluation data. In this section would use LogisticRegression to predict the class. We can use the default construction of LogisticRegression or tune some of the input parameters. I tried tuning various parameters for the model but the default gave better results. So leaving the initiatilization to default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logmodel = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X=train_data_trans, y=train_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is ready lets predict the classes on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the target classification using the logistic model \n",
    "log_predictions_on_test = logmodel.predict(X=test_data_tran)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting confusion matrix. \n",
    "\n",
    "#print(classification_report(log_predictions_on_test, test_target_data))\n",
    "#conf = confusion_matrix(y_pred=log_predictions_on_test, y_true=test_target_data)\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#sns.heatmap(conf)\n",
    "\n",
    "#As the number of classes are high confusion matrix display is not viewable. Hence commeted out the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets calculate the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89310180022303653"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate the accuracy of the model\n",
    "accuracy_score(y_pred=log_predictions_on_test, y_true=test_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Accuracy of the model on the test data is 89%. But this can be misleading sometimes. Its time to check the precision and recall metrics\n",
    "\n",
    "As it is a multiclass classification lets use the average='weighted' in calculation of precision and recall as it helps in calculating metrics for each label, and find their average, weighted by the number of true instances for each label\n",
    "\n",
    "In accuracy we don't have this problem as it is just the sum of all diagonal elements divided by total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.899311134834\n",
      "0.893101800223\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "precision = precision_score(y_pred=log_predictions_on_test, y_true=test_target_data, average='weighted')\n",
    "print(precision)\n",
    "recall = recall_score(y_pred=log_predictions_on_test, y_true=test_target_data, average='weighted')\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and Recall are also 89%. \n",
    "\n",
    "Its time to apply the model on the evaluation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_predictions_on_eval = logmodel.predict(X=eval_data_tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predictions for the evaluation data we need to merge these predictions with the original eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the predictions into dataframe before merging them \n",
    "log_eval_target = pd.DataFrame(data=log_predictions_on_eval, columns=[\"Predictions\"])\n",
    "log_final_eval_data = pd.concat([eval_raw, log_eval_target], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the predictions with the original eval data, dump all the data into \"Predictions.csv\" file under \"data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_final_eval_data.to_csv(\"data\\Predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the accuracy and recall by uploading the \"Predictions.csv\" at http://172.16.0.12:3838/\n",
    "\n",
    "Got an accuracy of 55.32 and Recall 69%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model using MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets to model using MultinomialNB as its generally good when there are multiple classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdbmodel = MultinomialNB()\n",
    "mdbmodel.fit(train_data_trans, train_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation is faster than the Logisitic Regression. Its time to predict the target values using this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_predictions = mdbmodel.predict(test_data_tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767086187669\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_pred=mnb_predictions, y_true=test_target_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got an accuracy of 77% . Check for precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.805110714839\n",
      "0.767086187669\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "precision = precision_score(y_pred=mnb_predictions, y_true=test_target_data, average='weighted')\n",
    "print(precision)\n",
    "recall = recall_score(y_pred=mnb_predictions, y_true=test_target_data, average='weighted')\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got recall of 81% and precision of 77% on the test data\n",
    "\n",
    "Lets predict the target for the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_predictions_on_eval = mdbmodel.predict(X=eval_data_tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predictions for the evaluation data we need to merge these predictions with the original eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_eval_target = pd.DataFrame(data=mnb_predictions_on_eval, columns=[\"Predictions\"])\n",
    "mnb_final_eval_data = pd.concat([eval_raw, mnb_eval_target], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the predictions with the original eval data, dump all the data into \"Predictions.csv\" file under \"data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_final_eval_data.to_csv(\"data\\Predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for the accuracy and recall by uploading the \"Predictions.csv\" at http://172.16.0.12:3838/\n",
    "\n",
    "Got an accuracy of 1 and Recall 9%. The model does not predict properly on the evaluation data even though the predictions on the test data seemed fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelling using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets to model using MultinomialNB as its generally good when there are many features SVM. SVM generally expects the number of training data set to be less. In our case we have good amount of training data. Lets try and see how the model works on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=20, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.2, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmmodel = SVC(C=20, gamma=0.2)\n",
    "svmmodel.fit(X=train_data_trans, y=train_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the model has taken lot of time to process. \n",
    "\n",
    "Time to do the prediction on test data using the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_predictions = svmmodel.predict(test_data_tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the accuracy, precision and recall metrics for the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy-', 0.81510983661692604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Precision - ', 0.8301432203386484)\n",
      "('Recall - ', 0.81510983661692604)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy-',accuracy_score(y_pred=svm_predictions, y_true=test_target_data))\n",
    "\n",
    "import numpy\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "precision = precision_score(y_pred=svm_predictions, y_true=test_target_data, average='weighted')\n",
    "print('Precision - ',precision)\n",
    "recall = recall_score(y_pred=svm_predictions, y_true=test_target_data, average='weighted')\n",
    "print('Recall - ', recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy, precision and Recall values are very less\n",
    "\n",
    "If these values are good we could tried the below steps. But it does not make sense to do as key metrics values are very low. Hence stopping this model processing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_predictions_on_eval = svmmodel.predict(X=eval_data_tran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the predictions with original data and after merging dump all the data into \"Predictions.csv\" file under \"data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_eval_target = pd.DataFrame(data=svm_predictions_on_eval, columns=[\"Predictions\"])\n",
    "svm_final_eval_data = pd.concat([eval_raw, svm_eval_target], axis = 1)\n",
    "svm_final_eval_data.to_csv(\"data\\Predictions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
